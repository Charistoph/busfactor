{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRP\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import overpy\n",
    "import pprint\n",
    "import geojson\n",
    "import time\n",
    "import pickle\n",
    "from h3 import h3\n",
    "from keplergl import KeplerGl\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# VRP\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_folder(folder_name: str):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return None\n",
    "\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = (np.sin(dlat / 2) ** 2\n",
    "         + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km * 1000\n",
    "\n",
    "def apply_hav(x):\n",
    "    try:\n",
    "        return haversine(float(x.start_lng), float(x.start_lat), \n",
    "                         float(x.end_lng), float(x.end_lat))\n",
    "    except:\n",
    "        print(x.start_lng, x.start_lat, x.end_lng, x.end_lat)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_json_name = 'routing_inputs_smol.json'\n",
    "input_json_name = 'routing_inputs.json'\n",
    "\n",
    "inputs_folder = 'inputs'\n",
    "output_folder = 'new_sol'\n",
    "dict_store = 'dicts'\n",
    "plots_path = os.path.join(output_folder, 'plots')\n",
    "dicts_path = os.path.join(output_folder, dict_store)\n",
    "try_create_folder(output_folder)\n",
    "try_create_folder(plots_path)\n",
    "try_create_folder(dicts_path)\n",
    "\n",
    "wh_df = pd.DataFrame([\n",
    "    ['wh0', 52.436, 13.376],\n",
    "    ['wh1', 52.561, 13.475]\n",
    "], columns=['name', 'lat', 'lng'])\n",
    "\n",
    "city_lat, city_lng = 52.52, 13.40\n",
    "len_half_window = 75\n",
    "reasonable_radius_margin = 0.05\n",
    "\n",
    "realistic_speed = {\n",
    "    3: 3,\n",
    "    5: 5,\n",
    "    6: 6,\n",
    "    7: 7,\n",
    "    10: 8,\n",
    "    15: 13,\n",
    "    17: 15,\n",
    "    20: 18,\n",
    "    30: 25,\n",
    "    40: 33,\n",
    "    50: 37,\n",
    "    60: 54,\n",
    "    70: 62,\n",
    "    80: 71,\n",
    "    100: 90,\n",
    "    130: 120\n",
    "}\n",
    "\n",
    "# key from this random dude on the internet that posted it online\n",
    "API_Key = \"AIzaSyBPXRMDhBvQYKmo_8wn3hWK3msfQmCluYw\"\n",
    "gmaps = googlemaps.Client(key=API_Key)\n",
    "\n",
    "distance_limit = 200\n",
    "\n",
    "time_penalty_per_scooter = 60*3\n",
    "\n",
    "# Berlin Bounding Box\n",
    "# NW 52.6716, 13.0875\n",
    "# SO 52.3923, 13.6858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download graph from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasonable_radius(wh_df, city_lng, city_lat, \n",
    "                          reasonable_radius_margin=0.05, verbose=False):\n",
    "    \"\"\"\n",
    "    calculate reasonable radius around city center\n",
    "    \"\"\"\n",
    "    reasonable_radius = 0\n",
    "    for idx, row in wh_df.iterrows():\n",
    "        dist = haversine(city_lng, city_lat, row.lng, row.lat)\n",
    "        if dist > reasonable_radius:\n",
    "            reasonable_radius = dist\n",
    "    reasonable_radius *= (1+reasonable_radius_margin)\n",
    "\n",
    "    assert reasonable_radius > 0\n",
    "    if verbose:\n",
    "        print('get_reasonable_radius: reasonable_radius: %i' % reasonable_radius)\n",
    "    return reasonable_radius\n",
    "\n",
    "\n",
    "def query_ways_in_radius(reasonable_radius, city_lat, city_lng, verbose=False):\n",
    "    \"\"\"\n",
    "    Load all ways within a radius around a center point from OSM (OpenStreetMap)\n",
    "    \"\"\"\n",
    "    api = overpy.Overpass()\n",
    "    if verbose:\n",
    "        print('query_ways_in_radius: start OSM query')\n",
    "        \n",
    "    start_time = time.time()\n",
    "    sql_statement = \"\"\"\n",
    "    way[highway](around:%i,%f,%f)[\"maxspeed\"]\n",
    "    ;(._;>;);\n",
    "    out meta;\n",
    "    \"\"\" % (reasonable_radius, city_lat, city_lng)\n",
    "    \n",
    "    if verbose:\n",
    "        print('query_ways_in_radius: sql_statement: %s' % sql_statement)\n",
    "        r = api.query(sql_statement)\n",
    "        print('query_ways_in_radius: OSM query finished after %0.2f sec' % (\n",
    "            time.time()-start_time))\n",
    "        print(len(r.nodes), len(r.ways), len(r.relations))\n",
    "    return r\n",
    "\n",
    "\n",
    "def retrieve_edges(r, verbose=False):\n",
    "    \"\"\"\n",
    "    retrieve edges from osm query result to create egdes dataframe\n",
    "    \"\"\"\n",
    "    edges_raw = []\n",
    "    num_nodes = 0\n",
    "    # iterate though ways\n",
    "    for way in r.ways:\n",
    "        # retrieve the way dictionary using the way.id\n",
    "        way = vars(r.get_way(way.id))\n",
    "    #     pprint.pprint(way)\n",
    "\n",
    "        # retrieve the identifier, type of street, maxspeed and the node ids\n",
    "        idf, typ = way['id'], way['tags']['highway']\n",
    "        maxspeed, node_ids = way['tags']['maxspeed'], way['_node_ids']\n",
    "    #     print(idf, typ, maxspeed)\n",
    "\n",
    "        # add the number of nodes to the nodes counter for later checks\n",
    "        num_nodes += (len(node_ids)-1)\n",
    "        # iterate through the nodes and add node pairs to edges array\n",
    "        # node pair = two nodes following each other\n",
    "        for i in range(len(node_ids)-1):\n",
    "            edges_raw.append(\n",
    "                # node_ids[:-1] = all node ids but the last\n",
    "                # node_ids[1:] = all node ids but the first\n",
    "                [node_ids[:-1][i], node_ids[1:][i], idf, typ, maxspeed])\n",
    "\n",
    "    # create edges df\n",
    "    edges_columns = ['start', 'end', 'way_id', 'type', 'maxspeed']\n",
    "    edges = pd.DataFrame(edges_raw, columns=edges_columns)\n",
    "    assert len(edges) == num_nodes\n",
    "    # check that the start and end point are never the same\n",
    "    assert len(edges[(edges['start']-edges['end']) == 0]) == 0\n",
    "    \n",
    "    if verbose:\n",
    "        print('retrieve_edges: len(edges): %i' % len(edges))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def create_nodes_from_edges(edges):\n",
    "    \"\"\"\n",
    "    create nodes df\n",
    "    \"\"\"\n",
    "    nodes_raw = np.unique(np.concatenate([edges['start'].values, edges['end'].values]))\n",
    "    nodes = pd.DataFrame(nodes_raw, columns=['id'])\n",
    "    nodes = nodes.assign(lat = nodes['id'].apply(lambda x: vars(r.get_node(x))['lat']))\n",
    "    nodes = nodes.assign(lng = nodes['id'].apply(lambda x: vars(r.get_node(x))['lon']))\n",
    "    nodes = nodes.assign(gh = nodes.apply(\n",
    "        lambda x: h3.geo_to_h3(float(x.lat), float(x.lng), 10), axis=1))\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def edges_add_geo_info(edges, nodes):\n",
    "    \"\"\"\n",
    "    Add geohash, lat, lng and dist to edges df.\n",
    "    dist = haversine distance between start and end coordinates of edge\n",
    "    \"\"\"\n",
    "    \n",
    "    # add gh to graph\n",
    "    node_to_gh = dict(zip(nodes.id, nodes.gh))\n",
    "    edges = edges.assign(start_gh = edges['start'].apply(lambda x: node_to_gh[x]))\n",
    "    edges = edges.assign(end_gh = edges['end'].apply(lambda x: node_to_gh[x]))\n",
    "\n",
    "    # add lat lng to graph\n",
    "    node_to_lat = dict(zip(nodes.id, nodes.lat))\n",
    "    node_to_lng = dict(zip(nodes.id, nodes.lng))\n",
    "    edges = edges.assign(start_lat = edges['start'].apply(lambda x: node_to_lat[x]))\n",
    "    edges = edges.assign(start_lng = edges['start'].apply(lambda x: node_to_lng[x]))\n",
    "    edges = edges.assign(end_lat = edges['end'].apply(lambda x: node_to_lat[x]))\n",
    "    edges = edges.assign(end_lng = edges['end'].apply(lambda x: node_to_lng[x]))\n",
    "\n",
    "    # calculate distances between edge start and end\n",
    "    print('edges_add_geo_info: start dist calculation')\n",
    "    start_time = time.time()\n",
    "    edges = edges.assign(dist =\n",
    "        edges.apply(lambda x: apply_hav(x), axis=1))\n",
    "    print('edges_add_geo_info: dist calculation finished after %0.2f sec' % (\n",
    "        time.time()-start_time))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "def make_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_maxspeed_int(edges):\n",
    "    edges = edges.assign(maxspeed = edges['maxspeed'].apply(\n",
    "        lambda x: make_int(x)))\n",
    "\n",
    "    init_len = len(edges)\n",
    "    # only one per mille of all edges are allowed to contain maxspeeds\n",
    "    # which are not convertable to int\n",
    "    # examples: word 'walk' or DE:urban instead of number (speed limit)\n",
    "    assert len(edges[edges['maxspeed'].isna()]) < len(edges)/1000\n",
    "    edges = edges[~edges['maxspeed'].isna()]\n",
    "    print('removed %i rows' % len(edges[edges['maxspeed'].isna()]))\n",
    "    print('new edges length: %i, (%0.2f%% of df before removal)' % (\n",
    "        len(edges), 100*len(edges)/init_len))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonable_radius = get_reasonable_radius(\n",
    "    wh_df, city_lng, city_lat, reasonable_radius_margin, True)\n",
    "\n",
    "r = query_ways_in_radius(\n",
    "    reasonable_radius, city_lat, city_lng, True)\n",
    "\n",
    "edges = retrieve_edges(r, True)\n",
    "\n",
    "\"\"\"\n",
    "# Some plots that are interesting but not cricital\n",
    "plt.hist(num_nodes, bins=100);\n",
    "\n",
    "num_intersects = []\n",
    "for start in edges['start'].values:\n",
    "    num_intersects.append(len(edges[edges['start'] == start]))\n",
    "plt.hist(num_intersects, bins=100);\n",
    "\"\"\"\n",
    "\n",
    "nodes = create_nodes_from_edges(edges)\n",
    "\n",
    "edges = edges_add_geo_info(edges, nodes)\n",
    "\n",
    "edges = convert_maxspeed_int(edges)\n",
    "\n",
    "nodes.to_csv(os.path.join(output_folder, \"nodes.csv\"))\n",
    "edges.to_csv(os.path.join(output_folder, \"edges.csv\"))\n",
    "\n",
    "\"\"\"\n",
    "# Save result of query to json\n",
    "# !!! doesnt work, might fix later !!!\n",
    "with open(os.path.join(\"res_berlin_25k.geo.json\"), mode=\"w\") as f:\n",
    "    geojson.dump(r, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holy shit that's allota nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_edges(edges, verbose=False):\n",
    "    init_len = len(edges)\n",
    "\n",
    "    # get reduced edges df\n",
    "    e = edges[['start', 'end', 'dist']]\n",
    "\n",
    "    # check that start and end point are never the same\n",
    "    assert len(e[(e['start']-e['end']) == 0]) == 0\n",
    "\n",
    "    # prepare for loop\n",
    "    min_dist = 25\n",
    "    dist_idx = e.columns.get_loc(\"dist\")\n",
    "    merged_pairs = []\n",
    "\n",
    "    if verbose:\n",
    "        print('merge_edges: start replace')\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(len(e[e['dist'] <= min_dist]))):\n",
    "        if e.iloc[i, dist_idx] <= min_dist:\n",
    "            old_value, new_value = e.iloc[i].end, e.iloc[i].start\n",
    "            if old_value != new_value:\n",
    "                merged_pairs.append([old_value, new_value])\n",
    "    #             print(\"old value: %i, new value: %i\" % (old_value, new_value))\n",
    "    #             display(e.loc[e['start'] == old_value])\n",
    "                e.loc[e['start'] == old_value, 'start'] = int(new_value)\n",
    "    #             display(e.loc[e['start'] == old_value])\n",
    "    #             display(e.loc[e['start'] == new_value])\n",
    "\n",
    "    #             display(e.loc[e['end'] == old_value])\n",
    "                e.loc[e['end'] == old_value, 'end'] = int(new_value)\n",
    "    #             display(e.loc[e['end'] == old_value])\n",
    "    #             display(e.loc[e['end'] == new_value])\n",
    "\n",
    "                assert len(e.loc[e['start'] == old_value]) == 0\n",
    "                assert len(e.loc[e['end'] == old_value]) == 0\n",
    "\n",
    "    # various checks\n",
    "    assert len(e) == len(edges)\n",
    "    assert len(e[e['start']-e['end'] != 0]) + len(e[(e['start']-e['end']) == 0]) == len(e)\n",
    "    # check that all start and ends points that were under and equal min_dist meters appart\n",
    "    # have the same start and end idxs\n",
    "    if verbose:\n",
    "        print(\"merge_edges: e[(e['start']-e['end']) == 0].dist.max()\", \n",
    "              e[(e['start']-e['end']) == 0].dist.max())\n",
    "    # assert e[(e['start']-e['end']) == 0].dist.max() <= min_dist\n",
    "    # check that all start and ends points that were over min_dist meters appart\n",
    "    # have different start and end idxs\n",
    "    if verbose:\n",
    "        print(\"merge_edges: e[e['start']-e['end'] != 0].dist.min()\", \n",
    "              e[e['start']-e['end'] != 0].dist.min())\n",
    "    assert e[e['start']-e['end'] != 0].dist.min() > min_dist\n",
    "\n",
    "    if verbose:\n",
    "        print('merge_edges: replace finished after %0.2f sec' % (\n",
    "            time.time()-start_time))\n",
    "\n",
    "    # overwrite edges\n",
    "    edges = e[['start', 'end']].join(edges[['way_id', 'type', 'maxspeed']])\n",
    "    # remove obsolete edges (where start and end point are the same)\n",
    "    edges = edges[edges['start']-edges['end'] != 0]\n",
    "\n",
    "    if verbose:\n",
    "        print('merge_edges: init_len: %i, len(remaining edges): %i (%0.4f%%)' % (\n",
    "            init_len, len(edges), 100*len(edges)/init_len))\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(os.path.join(output_folder, \"edges.csv\"), index_col=0)\n",
    "edges = edges.sort_values('dist')\n",
    "\n",
    "edges = merge_edges(edges, True)\n",
    "\n",
    "# lat lng has to be calculated again -> let's revisit our nodes\n",
    "nodes = pd.read_csv(os.path.join(output_folder, \"nodes.csv\"), index_col=0)\n",
    "nodes = nodes[['id', 'lat', 'lng']]\n",
    "\n",
    "\n",
    "# if verbose:\n",
    "print('start merge nodes')\n",
    "start_time = time.time()\n",
    "\n",
    "# merge node coordinates\n",
    "merged_pairs = pd.DataFrame(merged_pairs, columns=['start', 'end'])\n",
    "# iterate through all merged pairs\n",
    "for idx, row in tqdm(merged_pairs.iterrows()):\n",
    "    # each merged pair has two node ids (start id and end id)\n",
    "    # for each pair look up all OTHER merged pairs where either\n",
    "    # the start id or start id of the current pair (row) matches\n",
    "    # with either the start or end id of the merged_pairs df\n",
    "    sel_rows = merged_pairs.loc[\n",
    "        (merged_pairs['start'] == row.start) |\n",
    "        (merged_pairs['start'] == row.end) |\n",
    "        (merged_pairs['end'] == row.start) |\n",
    "        (merged_pairs['end'] == row.end)]\n",
    "#     print(sel_rows.index)\n",
    "    # get the indices of all merged pairs where the start or end id was matched\n",
    "    idxs = np.unique(sel_rows.values)\n",
    "    # get the nodes associated with the indices\n",
    "    n = nodes[nodes['id'].isin(idxs)]\n",
    "\n",
    "    # calculate the lat & lng mean of all the nodes that are being merged\n",
    "    # and write the means to all selected node rows (= now all nodes we\n",
    "    # wanted to merge have the same lat & lng)\n",
    "    nodes.loc[nodes['id'].isin(idxs), 'lat'] = n.lat.mean()\n",
    "    nodes.loc[nodes['id'].isin(idxs), 'lng'] = n.lng.mean()\n",
    "    \n",
    "    # drop the pairs which were merged in this iteration\n",
    "    merged_pairs = merged_pairs.drop(index=sel_rows.index)\n",
    "#     print(len(merged_pairs))\n",
    "\n",
    "# if verbose:\n",
    "print('merge nodes finished after %0.2f sec' % (\n",
    "    time.time()-start_time))\n",
    "\n",
    "\n",
    "# there are a lot of redundant nodes in the nodes df now\n",
    "# remove nodes which are not in the edges df anymore\n",
    "init_len_nodes = len(nodes)\n",
    "nodes = nodes[nodes['id'].isin(np.unique(np.concatenate(\n",
    "    [edges['start'].values, edges['end'].values])))]\n",
    "# print(init_len_nodes, len(nodes))\n",
    "\n",
    "# add gh to graph\n",
    "nodes = nodes.assign(gh = nodes.apply(\n",
    "    lambda x: h3.geo_to_h3(float(x.lat), float(x.lng), 10), axis=1))\n",
    "node_to_gh = dict(zip(nodes.id, nodes.gh))\n",
    "edges = edges.assign(start_gh = edges['start'].apply(lambda x: node_to_gh[x]))\n",
    "edges = edges.assign(end_gh = edges['end'].apply(lambda x: node_to_gh[x]))\n",
    "\n",
    "# add lat lng to graph\n",
    "node_to_lat = dict(zip(nodes.id, nodes.lat))\n",
    "node_to_lng = dict(zip(nodes.id, nodes.lng))\n",
    "edges = edges.assign(start_lat = edges['start'].apply(lambda x: node_to_lat[x]))\n",
    "edges = edges.assign(start_lng = edges['start'].apply(lambda x: node_to_lng[x]))\n",
    "edges = edges.assign(end_lat = edges['end'].apply(lambda x: node_to_lat[x]))\n",
    "edges = edges.assign(end_lng = edges['end'].apply(lambda x: node_to_lng[x]))\n",
    "\n",
    "# calculate distances between edge start and end\n",
    "print('start dist calculation')\n",
    "start_time = time.time()\n",
    "edges = edges.assign(dist =\n",
    "    edges.apply(lambda x: apply_hav(x), axis=1))\n",
    "print('dist calculation finished after %0.2f sec' % (\n",
    "    time.time()-start_time))\n",
    "\n",
    "# add realistic speed column\n",
    "# the realistic_speed dict was defined in the parameters above\n",
    "# some maxspeeds are not in the dictionary, we will assume that\n",
    "# the maxspeed equals the realistic speed in these cases\n",
    "for item in edges.maxspeed.unique():\n",
    "    if item not in realistic_speed.keys():\n",
    "        realistic_speed[item] = item\n",
    "print('realistic_speed dict:')\n",
    "pprint.pprint(realistic_speed)\n",
    "edges = edges.assign(realspeed = edges['maxspeed'].apply(lambda x: realistic_speed[x]))\n",
    "edges = edges.assign(drive_time = edges['dist']/edges['realspeed'])\n",
    "\n",
    "nodes.to_csv(os.path.join(output_folder, \"nodes_distilled.csv\"))\n",
    "edges.to_csv(os.path.join(output_folder, \"edges_distilled.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_1 = KeplerGl(height=500, data={\n",
    "#     'data_1': edges\n",
    "# }) # , config=config\n",
    "# map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the distance between each node using Dijkstra’s Shortest Path Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/cantors-paradise/dijkstras-shortest-path-algorithm-in-python-d955744c7064\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, data, indexloc=None):\n",
    "        self.data = data\n",
    "        self.index = indexloc\n",
    "\n",
    "\n",
    "class BinaryTree:\n",
    "\n",
    "    def __init__(self, nodes=[]):\n",
    "        self.nodes = nodes\n",
    "\n",
    "    def root(self):\n",
    "        return self.nodes[0]\n",
    "\n",
    "    def iparent(self, i):\n",
    "        return (i - 1) // 2\n",
    "\n",
    "    def ileft(self, i):\n",
    "        return 2 * i + 1\n",
    "\n",
    "    def iright(self, i):\n",
    "        return 2 * i + 2\n",
    "\n",
    "    def left(self, i):\n",
    "        return self.node_at_index(self.ileft(i))\n",
    "\n",
    "    def right(self, i):\n",
    "        return self.node_at_index(self.iright(i))\n",
    "\n",
    "    def parent(self, i):\n",
    "        return self.node_at_index(self.iparent(i))\n",
    "\n",
    "    def node_at_index(self, i):\n",
    "        return self.nodes[i]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.nodes)\n",
    "\n",
    "\n",
    "class DijkstraNodeDecorator:\n",
    "\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.prov_dist = float('inf')\n",
    "        self.hops = []\n",
    "\n",
    "    def index(self):\n",
    "        return self.node.index\n",
    "\n",
    "    def data(self):\n",
    "        return self.node.data\n",
    "\n",
    "    def update_data(self, data):\n",
    "        self.prov_dist = data['prov_dist']\n",
    "        self.hops = data['hops']\n",
    "        return self\n",
    "\n",
    "\n",
    "class MinHeap(BinaryTree):\n",
    "\n",
    "    def __init__(self, nodes, is_less_than=lambda a, b: a < b, get_index=None,\n",
    "                 update_node=lambda node, newval: newval):\n",
    "        BinaryTree.__init__(self, nodes)\n",
    "        self.order_mapping = list(range(len(nodes)))\n",
    "        self.is_less_than, self.get_index, self.update_node = is_less_than, get_index, update_node\n",
    "        self.min_heapify()\n",
    "\n",
    "    # Heapify at a node assuming all subtrees are heapified\n",
    "    def min_heapify_subtree(self, i):\n",
    "\n",
    "        size = self.size()\n",
    "        ileft = self.ileft(i)\n",
    "        iright = self.iright(i)\n",
    "        imin = i\n",
    "        if ileft < size and self.is_less_than(self.nodes[ileft], self.nodes[imin]):\n",
    "            imin = ileft\n",
    "        if (iright < size and self.is_less_than(self.nodes[iright], self.nodes[imin])):\n",
    "            imin = iright\n",
    "        if (imin != i):\n",
    "            self.nodes[i], self.nodes[imin] = self.nodes[imin], self.nodes[i]\n",
    "            # If there is a lambda to get absolute index of a node\n",
    "            # update your order_mapping array to indicate where that index lives\n",
    "            # in the nodes array (so lookup by this index is O(1))\n",
    "            if self.get_index is not None:\n",
    "                self.order_mapping[self.get_index(self.nodes[imin])] = imin\n",
    "                self.order_mapping[self.get_index(self.nodes[i])] = i\n",
    "            self.min_heapify_subtree(imin)\n",
    "\n",
    "    # Heapify an un-heapified array\n",
    "    def min_heapify(self):\n",
    "        for i in range(len(self.nodes), -1, -1):\n",
    "            self.min_heapify_subtree(i)\n",
    "\n",
    "    def min(self):\n",
    "        return self.nodes[0]\n",
    "\n",
    "    def pop(self):\n",
    "        min = self.nodes[0]\n",
    "        if self.size() > 1:\n",
    "            self.nodes[0] = self.nodes[-1]\n",
    "            self.nodes.pop()\n",
    "            # Update order_mapping if applicable\n",
    "            if self.get_index is not None:\n",
    "                self.order_mapping[self.get_index(self.nodes[0])] = 0\n",
    "            self.min_heapify_subtree(0)\n",
    "        elif self.size() == 1:\n",
    "            self.nodes.pop()\n",
    "        else:\n",
    "            return None\n",
    "        # If self.get_index exists, update self.order_mapping to indicate\n",
    "        # the node of this index is no longer in the heap\n",
    "        if self.get_index is not None:\n",
    "            # Set value in self.order_mapping to None to indicate it is not in the heap\n",
    "            self.order_mapping[self.get_index(min)] = None\n",
    "        return min\n",
    "\n",
    "    # Update node value, bubble it up as necessary to maintain heap property\n",
    "    def decrease_key(self, i, val):\n",
    "        self.nodes[i] = self.update_node(self.nodes[i], val)\n",
    "        iparent = self.iparent(i)\n",
    "        while i != 0 and not self.is_less_than(self.nodes[iparent], self.nodes[i]):\n",
    "            self.nodes[iparent], self.nodes[i] = self.nodes[i], self.nodes[iparent]\n",
    "            # If there is a lambda to get absolute index of a node\n",
    "            # update your order_mapping array to indicate where that index lives\n",
    "            # in the nodes array (so lookup by this index is O(1))\n",
    "            if self.get_index is not None:\n",
    "                self.order_mapping[self.get_index(\n",
    "                    self.nodes[iparent])] = iparent\n",
    "                self.order_mapping[self.get_index(self.nodes[i])] = i\n",
    "            i = iparent\n",
    "            iparent = self.iparent(i) if i > 0 else None\n",
    "\n",
    "    def index_of_node_at(self, i):\n",
    "        return self.get_index(self.nodes[i])\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, nodes):\n",
    "        self.adj_list = [[node, []] for node in nodes]\n",
    "        for i in range(len(nodes)):\n",
    "            nodes[i].index = i\n",
    "\n",
    "    def connect_dir(self, node1, node2, weight=1):\n",
    "        node1, node2 = self.get_index_from_node(\n",
    "            node1), self.get_index_from_node(node2)\n",
    "        # Note that the below doesn't protect from adding a connection twice\n",
    "        self.adj_list[node1][1].append((node2, weight))\n",
    "\n",
    "    def connect(self, node1, node2, weight=1):\n",
    "        self.connect_dir(node1, node2, weight)\n",
    "        self.connect_dir(node2, node1, weight)\n",
    "\n",
    "    def connections(self, node):\n",
    "        node = self.get_index_from_node(node)\n",
    "        return self.adj_list[node][1]\n",
    "\n",
    "    def get_index_from_node(self, node):\n",
    "        if not isinstance(node, Node) and not isinstance(node, int):\n",
    "            raise ValueError(\"node must be an integer or a Node object\")\n",
    "        if isinstance(node, int):\n",
    "            return node\n",
    "        else:\n",
    "            return node.index\n",
    "\n",
    "    def dijkstra(self, src):\n",
    "\n",
    "        src_index = self.get_index_from_node(src)\n",
    "        # Map nodes to DijkstraNodeDecorators\n",
    "        # This will initialize all provisional distances to infinity\n",
    "        dnodes = [DijkstraNodeDecorator(node_edges[0])\n",
    "                  for node_edges in self.adj_list]\n",
    "        # Set the source node provisional distance to 0 and its hops array to its node\n",
    "        dnodes[src_index].prov_dist = 0\n",
    "        dnodes[src_index].hops.append(dnodes[src_index].node)\n",
    "        # Set up all heap customization methods\n",
    "        def is_less_than(a, b): return a.prov_dist < b.prov_dist\n",
    "        def get_index(node): return node.index()\n",
    "        def update_node(node, data): return node.update_data(data)\n",
    "\n",
    "        # Instantiate heap to work with DijkstraNodeDecorators as the hep nodes\n",
    "        heap = MinHeap(dnodes, is_less_than, get_index, update_node)\n",
    "\n",
    "        min_dist_list = []\n",
    "\n",
    "        while heap.size() > 0:\n",
    "            # Get node in heap that has not yet been \"seen\"\n",
    "            # that has smallest distance to starting node\n",
    "            min_decorated_node = heap.pop()\n",
    "            min_dist = min_decorated_node.prov_dist\n",
    "            hops = min_decorated_node.hops\n",
    "            min_dist_list.append([min_dist, hops])\n",
    "\n",
    "            # Get all next hops. This is no longer an O(n^2) operation\n",
    "            connections = self.connections(min_decorated_node.node)\n",
    "            # For each connection, update its path and total distance from\n",
    "            # starting node if the total distance is less than the current distance\n",
    "            # in dist array\n",
    "            for (inode, weight) in connections:\n",
    "                node = self.adj_list[inode][0]\n",
    "                heap_location = heap.order_mapping[inode]\n",
    "                if heap_location is not None:\n",
    "                    tot_dist = weight + min_dist\n",
    "                    if tot_dist < heap.nodes[heap_location].prov_dist:\n",
    "                        hops_cpy = list(hops)\n",
    "                        hops_cpy.append(node)\n",
    "                        data = {'prov_dist': tot_dist, 'hops': hops_cpy}\n",
    "                        heap.decrease_key(heap_location, data)\n",
    "\n",
    "        return min_dist_list\n",
    "\n",
    "\"\"\"\n",
    "# THE LINES BELOW ARE TO TEST Dijkstra’s Shortest Path Algorithm\n",
    "\n",
    "a = Node('a')\n",
    "b = Node('b')\n",
    "c = Node('c')\n",
    "d = Node('d')\n",
    "e = Node('e')\n",
    "f = Node('f')\n",
    "\n",
    "g = Graph([a,b,c,d,e,f])\n",
    "\n",
    "g.connect(a,b,5)\n",
    "g.connect(a,c,10)\n",
    "g.connect(a,e,2)\n",
    "g.connect(b,c,2)\n",
    "g.connect(b,d,4)\n",
    "g.connect(c,d,7)\n",
    "g.connect(c,f,10)\n",
    "g.connect(d,e,3)\n",
    "\n",
    "print([(weight, [n.data for n in node]) for (weight, node) in g.dijkstra(a)])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS UNINTENTIONALLY\n",
    "edges = pd.read_csv(os.path.join(output_folder, \"edges_distilled.csv\"), index_col=0)\n",
    "nodes = pd.read_csv(os.path.join(output_folder, \"nodes_distilled.csv\"), index_col=0)\n",
    "\n",
    "# TODO: fix this!! THE NODES SHOULD MATCH!!!!!!\n",
    "nodes = nodes[nodes['id'].isin(np.unique(np.concatenate(\n",
    "    [edges['start'].values, edges['end'].values])))]\n",
    "assert len(np.unique(np.concatenate([edges['start'].values, edges['end'].values]))) == len(nodes)\n",
    "# TODO: fix this!! THERE SHOULD BE NO DUPLICATES!!!!!!\n",
    "edges = edges.loc[edges[['start', 'end']].drop_duplicates().index].reset_index(drop=True)\n",
    "nodes.to_csv(os.path.join(output_folder, \"nodes_final.csv\"))\n",
    "edges.to_csv(os.path.join(output_folder, \"edges_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {}\n",
    "node_arr = []\n",
    "nodes = nodes.reset_index(drop=True)\n",
    "for idx, node in nodes.iterrows():\n",
    "    node_dict[node.id] = idx\n",
    "    node_arr.append(Node(node.id))\n",
    "\n",
    "g = Graph(node_arr)\n",
    "\n",
    "for idx, edge in edges.iterrows():\n",
    "    g.connect(node_arr[node_dict[edge.start]], node_arr[node_dict[edge.end]], edge.drive_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WARNING WARNING WARNING, THIS TAKES 15h TO RUN!!!\n",
    "# \"\"\"\n",
    "# This soon to be function iterates over all nodes and calculates\n",
    "# the drive time between to all other node for each node. It saves\n",
    "# the output in dictionaries in the following manner:\n",
    "# for current_node in nodes:\n",
    "#     for other_node in nodes:\n",
    "#     dict[current_node][other_node] = drive_time\n",
    "# It it runs for approx 1.5 secs per node (for berlin it takes 15h to run).\n",
    "# \"\"\"\n",
    "# file_idx = 0\n",
    "# drive_dict = dict()\n",
    "# i = 0\n",
    "# for curr_id in tqdm(nodes.id.values):\n",
    "#     arr = g.dijkstra(node_arr[node_dict[curr_id]])\n",
    "    \n",
    "#     # for idx, item in enumerate(arr):\n",
    "#     for item in arr:\n",
    "#     #     try:\n",
    "#         weight, path = item\n",
    "#         if weight != np.inf:\n",
    "#             last_node = path[-1].data\n",
    "#             try:\n",
    "#                 drive_dict[curr_id][last_node] = weight\n",
    "#             except:\n",
    "#                 try:\n",
    "#                     drive_dict[last_node][curr_id] = weight\n",
    "#                 except:\n",
    "#                     drive_dict[curr_id] = dict()\n",
    "#                     drive_dict[curr_id][last_node] = weight\n",
    "#     if i % 500 == 1:\n",
    "#         f = open(os.path.join(dicts_path, 'drive_dict-%i.csv' % file_idx), \"wb\")\n",
    "#         pickle.dump(drive_dict, f)\n",
    "#         f.close()\n",
    "#         drive_dict = dict()\n",
    "#         file_idx += 1\n",
    "#     i += 1\n",
    "#     #     except:\n",
    "#     #         print(idx, item)\n",
    "#     #         asdf\n",
    "    \n",
    "# f = open(os.path.join(dicts_path, 'drive_dict-%i.csv' % file_idx), \"wb\")\n",
    "# pickle.dump(drive_dict, f)\n",
    "# f.close()\n",
    "# drive_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nodes_dict(dict_names):\n",
    "    \"\"\"\n",
    "    This function checks which keys are in the dictionaries\n",
    "    to assess if drive time entries were created for all nodes.\n",
    "    \"\"\"\n",
    "    nodes_in_dict = []\n",
    "    for dict_name in tqdm(dict_names):\n",
    "        f = open(os.path.join(dicts_path, dict_name), 'rb')\n",
    "        d = pickle.load(f)\n",
    "        f.close()\n",
    "        if type(d) == dict:\n",
    "            nodes_in_dict.extend(d.keys())\n",
    "    return np.array(nodes_in_dict)\n",
    "\n",
    "dict_names = np.sort(np.array([f for f in os.listdir(dicts_path) if 'drive_dict' in f]))\n",
    "print('dict_names:', dict_names)\n",
    "nodes_in_dict = check_nodes_dict(dict_names) \n",
    "# check that there are no duplicates in the dict\n",
    "assert len(np.unique(nodes_in_dict)) == len(nodes_in_dict)\n",
    "# check that all nodes are in the dict\n",
    "assert (np.sort(nodes_in_dict)-nodes['id'].sort_values().values).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_are_my_nodes(dict_names):\n",
    "    where_nodes_dict = dict()\n",
    "    for dict_name in tqdm(dict_names):\n",
    "        f = open(os.path.join(dicts_path, dict_name), 'rb')\n",
    "        d = pickle.load(f)\n",
    "        f.close()\n",
    "        if type(d) == dict:\n",
    "            for key in d.keys():\n",
    "                where_nodes_dict[key] = dict_name\n",
    "    return where_nodes_dict\n",
    "\n",
    "where_nodes_dict = where_are_my_nodes(dict_names)\n",
    "f = open(os.path.join(dicts_path, 'where_nodes_dict.csv'), \"wb\")\n",
    "pickle.dump(where_nodes_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you should have by now:\n",
    "\n",
    "- nodes_final.csv - Contains all nodes of the city graph. The city graph is big enough to enclose all warehouses in the city\n",
    "- edges_final.csv - The edges connecting the nodes including columns like 'gh', 'lat/lng', 'type', 'maxspeed', 'dist', 'realspeed',  'drive_time'\n",
    "- a dicts folder inside the output (currently named new sol) folder. These dictionaries contain the drive times from every node to every other node (at the moment 30000^2 ~= 10^8 key value pairs)\n",
    "- where_nodes_dict.csv (inside dict folder) - This dictionary contains a complete mapping of which node and its corresponding drive times is saved in which dict file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
